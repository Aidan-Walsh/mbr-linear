{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJoiGJdSqOSl",
        "outputId": "9ca776ec-02de-4769-ae08-5125eb88961a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.6 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge_metric\n",
            "  Downloading rouge_metric-1.0.1-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rouge_metric\n",
            "Successfully installed rouge_metric-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install rouge_metric\n",
        "!pip install transformers\n",
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# !cd drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8coBKzNA9kge"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "\n",
        "import collections\n",
        "import json\n",
        "import os\n",
        "import regex\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MBPPGoogleDataset(object): # may have to change this path \n",
        "    def __init__(self, path='/content/drive/Shareddrives/COS484FinalProject/data/mbpp/mbpp.jsonl', mode='function_name'):\n",
        "        raw_data = sorted([json.loads(x) for x in open(path)], key=lambda x: x['task_id'])\n",
        "        for i, data_item in enumerate(raw_data):\n",
        "            assert data_item['task_id'] == i + 1\n",
        "        self.raw_data = collections.defaultdict()\n",
        "        self.mode = mode\n",
        "        # 374 for training, 100 heldout, 500 test\n",
        "        self.raw_data['train'] = raw_data[:10] + raw_data[510:]\n",
        "        self.raw_data['test'] = raw_data[10:510]\n",
        "        # data for codex collector, in input-output-info format\n",
        "        self.data = collections.defaultdict()\n",
        "        for split in self.raw_data:\n",
        "            self.data[split] = self.extract_data(self.raw_data[split], mode)\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_data(raw_data, mode):\n",
        "        if mode == 'function_name':\n",
        "            get_function_name = lambda test_example: regex.match('assert [\\(]*([^\\(]+)\\(', test_example).group(1)\n",
        "            info = [get_function_name(x['test_list'][0]) for x in raw_data]\n",
        "        elif mode == 'assertion':\n",
        "            info = [x['test_list'][0] for x in raw_data]\n",
        "        elif mode == 'assertion-full':\n",
        "            info = [x['test_list'] for x in raw_data]\n",
        "        else:\n",
        "            raise Exception(f'Mode {mode} not supported.')\n",
        "        nls = [x['text'] for x in raw_data]\n",
        "        codes = [x['code'] for x in raw_data]\n",
        "        return list(zip(nls, codes, info))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qUsWG9rWz3aH"
      },
      "outputs": [],
      "source": [
        "class Object(object):\n",
        "  pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ngI77YjtEVhO"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "\n",
        "import argparse\n",
        "import copy\n",
        "import json\n",
        "import openai\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import signal\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from glob import glob\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import single_meteor_score as m\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import numpy as np\n",
        "from rouge_metric import PyRouge\n",
        "import nltk\n",
        "\n",
        "\n",
        "def codex_with_info(configs, dataset, prefixes, tokenizer, model):\n",
        "\n",
        "    def codex_greedy(prompt):\n",
        "        inputs = tokenizer(text=prompt, truncation=True,\n",
        "                           return_tensors=\"pt\").input_ids\n",
        "\n",
        "        sample = model.generate(inputs, temperature=0, output_scores=True,\n",
        "                                return_dict_in_generate=True, max_new_tokens=200, renormalize_logits=True)\n",
        "\n",
        "        output = tokenizer.decode(sample[0][0])\n",
        "\n",
        "        \n",
        "        # return this to get text\n",
        "        text = output[len(prompt):].split(\"</code>\", 1)[0]\n",
        "        #print(text)\n",
        "        return text, None, None\n",
        "    \n",
        "\n",
        "    def codex_sample(prompt):\n",
        "        inputs = tokenizer(text=prompt, truncation=True,\n",
        "                           return_tensors=\"pt\").input_ids\n",
        "\n",
        "        sample = model.generate(inputs, temperature=configs.temperature, output_scores=True,\n",
        "                                return_dict_in_generate=True, do_sample=True, max_new_tokens=200, renormalize_logits=True)\n",
        "\n",
        "        output = tokenizer.decode(sample[0][0])\n",
        "\n",
        "        # return this to get text\n",
        "        text = output[len(prompt):].split(\"</code>\", 1)[0]\n",
        "        # return this to return token ids\n",
        "        encoded = tokenizer.encode(text)\n",
        "        enc_len = len(encoded)\n",
        "\n",
        "        log_probs = np.zeros(enc_len)\n",
        "        # return this as final logs\n",
        "        for i in range(enc_len):\n",
        "            log_probs[i] = sample[1][i][0][encoded[i]]\n",
        "        return text, encoded, log_probs.tolist()\n",
        "\n",
        "\n",
        "    prompt_prefix = ''.join([configs.prompt_template.format(\n",
        "        src=x[0], trg=x[1], info=x[2]) for x in prefixes])\n",
        "\n",
        "    # save folder\n",
        "    save_dir = f'{configs.output_path}/seed-{configs.seed}/{configs.n_prompts}-shot/{configs.mode}-{configs.temperature}/'\n",
        "    \n",
        "    os.system(f'mkdir -p {save_dir}')\n",
        "    # save configs and prefixes\n",
        "    if configs.rank == 0:\n",
        "        with open(f'{save_dir}/prefixes.json', 'w') as fout:\n",
        "            json.dump(prefixes, fout)\n",
        "            fout.close()\n",
        "        with open(f'{save_dir}/configs.pkl', 'wb') as fout:\n",
        "            pickle.dump(configs, fout)\n",
        "            fout.close()\n",
        "    ofname = f'{save_dir}/{configs.split}-{configs.rank}.jsonl'\n",
        "    # load checkpoint\n",
        "    if os.path.exists(ofname):\n",
        "        n_processed_examples = len(open(ofname).readlines())\n",
        "    else:\n",
        "        n_processed_examples = 0\n",
        "    pbar = tqdm(dataset)\n",
        "\n",
        "    with open(ofname, 'a') as fout:\n",
        "        for i, (src, trg, info) in enumerate(pbar):\n",
        "            if i < n_processed_examples:\n",
        "                continue\n",
        "            prompt = prompt_prefix + \\\n",
        "                configs.example_template.format(src=src, info=info)\n",
        "            print(\"prompt\")\n",
        "            print(prompt)\n",
        "            while True:\n",
        "                try:\n",
        "                    trg_prediction, tokens, logprobs = codex_greedy(\n",
        "                        prompt) if configs.mode == 'greedy' else codex_sample(prompt)\n",
        "                    time.sleep(2)\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    \n",
        "                    print(e, flush=True)\n",
        "                    time.sleep(3)\n",
        "            try:\n",
        "\n",
        "                bleu_score = sentence_bleu([[ch for ch in trg]], [\n",
        "                                           ch for ch in trg_prediction])\n",
        "            \n",
        "            except:\n",
        "                print(\"fail bleu\")\n",
        "                bleu_score = 0\n",
        "\n",
        "            try:\n",
        "                token_bleu_score = sentence_bleu([trg.split()], trg_prediction.split())\n",
        "                print(\"token bleu: \", token_bleu_score)\n",
        "            except:\n",
        "                token_bleu_score = 0\n",
        "            \n",
        "            try: \n",
        "\n",
        "                rouge_score = 0\n",
        "                rouge = PyRouge(rouge_n=(1,), rouge_l=True,\n",
        "                                    rouge_w=True, rouge_w_weight=1.2, rouge_s=True)\n",
        "\n",
        "                rouge_score = rouge.evaluate_tokenized(\n",
        "                        [[ch for ch in trg_prediction]], [[ch for ch in trg]])\n",
        "               \n",
        "\n",
        "            except:\n",
        "                print(\"fail rouge\")\n",
        "                rouge_score = 0\n",
        "\n",
        "            try: \n",
        "                \n",
        "                \n",
        "                \n",
        "                meteor_score = m([ch for ch in trg], [ch for ch in trg_prediction])\n",
        "                \n",
        "                \n",
        "\n",
        "            except: \n",
        "                print(\"fail meteor\")\n",
        "                \n",
        "                \n",
        "                meteor_score = 0\n",
        "\n",
        "            print(\n",
        "                json.dumps(\n",
        "                    {\n",
        "                        'prompt': prompt,\n",
        "                        'src': src,\n",
        "                        'trg_prediction': trg_prediction,\n",
        "                        'reference': trg,\n",
        "                        'tokens': tokens,\n",
        "                        'logprobs': logprobs,\n",
        "                        'bleu': bleu_score,\n",
        "                        'rouge': rouge_score, \n",
        "                        'meteor': meteor_score,\n",
        "                        'token_bleu': token_bleu_score\n",
        "                        \n",
        "                    }\n",
        "                ),\n",
        "                file=fout, flush=True\n",
        "            )\n",
        "            pbar.set_description(f'Process {configs.rank}')\n",
        "        fout.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "# not needed to run interpolation since this just collects data, which can be \n",
        "# done by using Baselines.ipynb\n",
        "# class CollectorWithInfo(object):\n",
        "#     def __init__(self, dataset, tokenizer, model, rank):\n",
        "#        #change here for arguments\n",
        "#       self.configs = Object()\n",
        "#       self.configs.output_path = \"/content/drive/MyDrive/data/mbr-2B/mbpp\"\n",
        "#       self.configs.split = \"test\"\n",
        "#       self.configs.seed = [1,2,3,4,5]\n",
        "#       self.configs.temperature = [0.3,0.6]\n",
        "#       self.configs.n_prompts = [3]\n",
        "#       self.configs.n_samples = 5 # number of seeds multiplied by number of samples is the total number that we can look at\n",
        "#       self.configs.mode = \"sample\"\n",
        "#       self.configs.prompt_template = \"<info>{info}</info>\\n<text>{src}</text>\\n<code>{trg}</code>\\n\"\n",
        "#       self.configs.example_template = \"<info>{info}</info>\\n<text>{src}</text>\\n<code>\"\n",
        "#       self.configs.end_template = \"</code>\"\n",
        "#       self.configs.shuffle_prefix = False\n",
        "#       self.configs.saved_prefixes_path_template = None\n",
        "#       self.configs.rank = rank\n",
        "#       self.dataset = dataset\n",
        "#       self.tokenizer = tokenizer\n",
        "#       self.model = model\n",
        "\n",
        "#     def __call__(self):\n",
        "\n",
        "        \n",
        "#         configs = copy.deepcopy(self.configs)\n",
        "        \n",
        "#         for seed in self.configs.seed:\n",
        "#             for n_prompts in self.configs.n_prompts:\n",
        "#                 for temperature in self.configs.temperature:\n",
        "#                     configs.n_prompts = n_prompts\n",
        "#                     configs.seed = seed\n",
        "#                     configs.temperature = temperature\n",
        "#                     random.seed(configs.seed)\n",
        "#                     if configs.saved_prefixes_path_template is not None:\n",
        "#                         prefix_pool = list()\n",
        "#                         for path in glob(configs.saved_prefixes_path_template, recursive=True):\n",
        "#                             prefix_pool.extend(json.load(open(path)))\n",
        "#                         prefix_pool = sorted(\n",
        "#                             set([tuple(x) for x in prefix_pool]))\n",
        "#                         prefixes = random.sample(\n",
        "#                             prefix_pool, configs.n_prompts)\n",
        "#                     else:\n",
        "#                         prefixes = random.sample(\n",
        "#                             self.dataset.data['train'], configs.n_prompts)\n",
        "#                     if configs.shuffle_prefix:\n",
        "#                         original_prefixes = copy.deepcopy(prefixes)\n",
        "#                         while original_prefixes == prefixes:\n",
        "#                             random.shuffle(prefixes)\n",
        "#                     codex_with_info(\n",
        "#                         configs, self.dataset.data[configs.split], prefixes, self.tokenizer, self.model)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qZKN9C55PyP",
        "outputId": "29ec1453-e122-444b-c781-c86e61c475ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
            "Successfully installed datasets-2.12.0 dill-0.3.6 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kM1VeRkkUnVy"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import regex\n",
        "import signal\n",
        "import subprocess\n",
        "import tempfile\n",
        "import threading\n",
        "from datasets import load_metric\n",
        "from glob import glob\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Command(object):\n",
        "    def __init__(self, cmd):\n",
        "        self.cmd = cmd\n",
        "        self.process = None\n",
        "\n",
        "    def run(self, timeout):\n",
        "        def target():\n",
        "            self.process = subprocess.Popen(self.cmd, shell=True, preexec_fn=os.setsid)\n",
        "            self.process.communicate()\n",
        "\n",
        "        thread = threading.Thread(target=target)\n",
        "        thread.start()\n",
        "\n",
        "        thread.join(timeout)\n",
        "        if thread.is_alive():\n",
        "            os.killpg(self.process.pid, signal.SIGTERM)\n",
        "            thread.join()\n",
        "        return self.process.returncode\n",
        "\n",
        "\n",
        "class PythonFunctionExecutor(object):\n",
        "    def __init__(self, function_content, function_call, timeout=10):\n",
        "        self.function_content = function_content\n",
        "        self.function_call = function_call\n",
        "        self.timeout = timeout\n",
        "\n",
        "    def __call__(self):\n",
        "        tempdir = tempfile.TemporaryDirectory()\n",
        "        #with open(f'{tempdir.name}/code.py', 'w') as fout:\n",
        "            # print(self.function_content, file=fout)\n",
        "            # print(f'result = {self.function_call}', file=fout)\n",
        "            # print(f'import pickle', file=fout)\n",
        "            # print(f'pickle.dump(result, open(\"{tempdir.name}/execution_result.pkl\", \"wb\"))', file=fout)\n",
        "        command = Command(f'python {tempdir.name}/code.py >/dev/null 2>&1')\n",
        "        execution_status = command.run(timeout=self.timeout)\n",
        "        if execution_status == 0:\n",
        "            try:\n",
        "                execution_results = pickle.load(open(f'{tempdir.name}/execution_result.pkl', 'rb'))\n",
        "            except:\n",
        "                execution_results = None\n",
        "        else:\n",
        "            execution_results = None\n",
        "        tempdir.cleanup()\n",
        "        return execution_status, execution_results\n",
        "\n",
        "\n",
        "def execute_mbpp_google_folder(base_path):\n",
        "    # single assertion\n",
        "\n",
        "    dataset = MBPPGoogleDataset(mode='assertion')\n",
        "\n",
        "    for path in glob(f'{base_path}/*jsonl'):  # execute first assertion call\n",
        "        \n",
        "        if os.path.exists(path.replace('jsonl', 'exec.pkl')):\n",
        "            continue\n",
        "        split = os.path.basename(path).split('-')[0]\n",
        "        execution_results = list()\n",
        "        for i, line in enumerate(tqdm(open(path).readlines())):\n",
        "            assertion = dataset.data[split][i][-1]\n",
        "            command = regex.match(f'assert (.+)==.+', assertion).group(1)\n",
        "            item = json.loads(line)\n",
        "            python_function = item['trg_prediction']\n",
        "            executor = PythonFunctionExecutor(python_function, command)\n",
        "            execution_result = executor()\n",
        "            execution_results.append(execution_result)\n",
        "        with open(path.replace('jsonl', 'exec.pkl'), 'wb') as fout:\n",
        "            pickle.dump(execution_results, fout)\n",
        "    # multiple assertions (cheating)\n",
        "    dataset = MBPPGoogleDataset(mode='assertion-full')\n",
        "    for path in glob(f'{base_path}/*jsonl'):  # execute all assertion calls\n",
        "        if os.path.exists(path.replace('jsonl', 'execfull.pkl')):\n",
        "            continue\n",
        "        split = os.path.basename(path).split('-')[0]\n",
        "        execution_results = list()\n",
        "        for i, line in enumerate(tqdm(open(path).readlines())):\n",
        "            execution_result = list()\n",
        "            item = json.loads(line)\n",
        "            python_function = item['trg_prediction']\n",
        "            for assertion in dataset.data[split][i][-1]:\n",
        "                command = regex.match(f'assert (.+)==.+', assertion).group(1)\n",
        "                executor = PythonFunctionExecutor(python_function, command)\n",
        "                execution_result.append(executor())\n",
        "            execution_results.append(execution_result)\n",
        "        with open(path.replace('jsonl', 'execfull.pkl'), 'wb') as fout:\n",
        "            pickle.dump(execution_results, fout)\n",
        "    # multiple assertions (pass or fail)\n",
        "    for path in glob(f'{base_path}/*jsonl'):\n",
        "        if os.path.exists(path.replace('jsonl', 'execfullpass.pkl')):\n",
        "            continue\n",
        "        split = os.path.basename(path).split('-')[0]\n",
        "        execution_results = list()\n",
        "        for i, line in enumerate(tqdm(open(path).readlines())):\n",
        "            execution_result = list()\n",
        "            item = json.loads(line)\n",
        "            python_function = item['trg_prediction']\n",
        "            for assertion in dataset.data[split][i][-1]:\n",
        "                command = regex.match(f'assert (.+==.+)', assertion).group(1)\n",
        "                executor = PythonFunctionExecutor(python_function, f'({command})')\n",
        "                execution_result.append(executor())\n",
        "            execution_results.append(execution_result)\n",
        "        with open(path.replace('jsonl', 'execfullpass.pkl'), 'wb') as fout:\n",
        "            pickle.dump(execution_results, fout)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hA4kLTIySZsF"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "from datasets import load_metric\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" dataset keys: src, trg_prediction, reference \"\"\"\n",
        "\n",
        "\n",
        "def evaluate_charbleu(dataset):\n",
        "    \n",
        "    bleu = load_metric('bleu')\n",
        "    predictions = [[ch for ch in item['trg_prediction']] for item in dataset]\n",
        "    references = [[[ch for ch in item['reference']]] for item in dataset]\n",
        "    return bleu.compute(predictions=predictions, references=references)\n",
        "\n",
        "\n",
        "\"\"\" dataset keys: src, trg_prediction, reference (only trg_prediction useful) \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_google_mbpp(dataset, reference_path, split='test', timeout=10, return_details=False, gridSearch=False):\n",
        "    references = MBPPGoogleDataset(reference_path)\n",
        "    assert len(dataset) == len(references.raw_data[split])\n",
        "    tempdir = tempfile.TemporaryDirectory()\n",
        "    passed_information = list()\n",
        "    pbar = tqdm(references.raw_data[split])\n",
        "    for i, item in enumerate(pbar):\n",
        "        if 'execution_result_full_pass' in dataset[i]:\n",
        "            passed_information.append(\n",
        "                int(all(x[1] == True for x in dataset[i]['execution_result_full_pass'])))\n",
        "        else:\n",
        "            test_cases = item['test_list']\n",
        "            test_setups = item['test_setup_code']\n",
        "            code = dataset[i]['trg_prediction']\n",
        "            # write code to file\n",
        "            with open(f'{tempdir.name}/code.py', 'w') as fout:\n",
        "                # print(code, file=fout)\n",
        "                # print(test_setups, file=fout)\n",
        "                # for case in test_cases:\n",
        "                #     print(case, file=fout)\n",
        "                fout.close()\n",
        "            command = Command(f'python {tempdir.name}/code.py >/dev/null 2>&1')\n",
        "            execution_result = (command.run(timeout=timeout) == 0)\n",
        "            passed_information.append(int(execution_result))\n",
        "        pbar.set_description(f'{sum(passed_information)} out of {i+1} passed.')\n",
        "    tempdir.cleanup()\n",
        "    if gridSearch:\n",
        "        return sum(passed_information), len(passed_information)\n",
        "    if return_details:\n",
        "        return passed_information\n",
        "    else:\n",
        "        return sum(passed_information) / len(passed_information)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bn767HWGYDHp"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "\n",
        "\n",
        "import collections\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from sklearn import preprocessing\n",
        "from rouge_metric import PyRouge\n",
        "\n",
        "\"\"\" k sets of configs in separate paths, n * k choose 1 selector \"\"\"\n",
        "\n",
        "\n",
        "class MultiSampleSelector(object):\n",
        "    def __init__(self, paths, split='dev', hyper_params=None):\n",
        "        self.paths = list(sorted(glob(paths, recursive=True))) if isinstance(\n",
        "            paths, str) else list(sorted(paths))\n",
        "        self.split = split\n",
        "        self.data = collections.defaultdict(list)\n",
        "        self.args = collections.defaultdict(list)\n",
        "        # self.hyper_params = np.array([100.0, 1.0, 100.0, 0.3])  # bleu, mbr, avg_log\n",
        "        self.hyper_params = hyper_params  # bleu, mbr, avg_log\n",
        "\n",
        "        for i, path in enumerate(self.paths):\n",
        "            self.args[i] = pickle.load(\n",
        "                open(f'{self.paths[0]}/configs.pkl', 'rb'))\n",
        "            idx = 0\n",
        "            while os.path.exists(f'{path}/{split}-{idx}.jsonl'):\n",
        "                self.data[i, idx].extend(\n",
        "                    [json.loads(x) for x in open(f'{path}/{split}-{idx}.jsonl')])\n",
        "                idx += 1\n",
        "        for path_id, sample_id in self.data:\n",
        "            for item in self.data[path_id, sample_id]:\n",
        "                try:\n",
        "                    avg_logprob, sum_logprob = self.extract_logprob_stats(\n",
        "                        item, path_id)\n",
        "                    item['avg_logprob'] = avg_logprob\n",
        "                    item['sum_logprob'] = sum_logprob\n",
        "\n",
        "                except:\n",
        "                    item['avg_logprob'] = item['sum_logprob'] = 0\n",
        "                \n",
        "    def extract_logprob_stats(self, item, path_id):\n",
        "        current_seq = ''\n",
        "        extracted_position = None\n",
        "        for i, _ in enumerate(item['tokens']):\n",
        "            current_seq += item['tokens'][i]\n",
        "            if current_seq.find(item['trg_prediction']) != -1 and current_seq.find(self.args[path_id].end_template) != -1:\n",
        "                extracted_position = i + 1\n",
        "                break\n",
        "        logprobs = item['logprobs'][:extracted_position] if extracted_position is not None else item['logprobs']\n",
        "        # handle potential codex bug on positive log probability\n",
        "        logprobs = list(filter(lambda x: x < 0, logprobs))\n",
        "        return np.mean(logprobs), np.sum(logprobs)\n",
        "\n",
        "    def select(self, ids=None, key_extractor=lambda x: x['avg_logprob'], return_keys=False):\n",
        "        if ids is None:\n",
        "            ids = self.data.keys()\n",
        "        ids = list(sorted(ids))\n",
        "        print(f'Selecting Samples from IDs: {ids}', flush=True)\n",
        "        n_examples = len(self.data[ids[0]])\n",
        "        selected_examples = list()\n",
        "        sample_keys = collections.defaultdict(list)\n",
        "        for i in range(n_examples):\n",
        "            max_key = None\n",
        "            selected_item = None\n",
        "            for idx in ids:\n",
        "                item = self.data[idx][i]\n",
        "                key = key_extractor(item)\n",
        "                sample_keys[idx].append(key)\n",
        "                if max_key is None or key > max_key:\n",
        "                    max_key = key\n",
        "                    selected_item = item\n",
        "            assert selected_item is not None\n",
        "            selected_examples.append(selected_item)\n",
        "        if return_keys:\n",
        "            return selected_examples, sample_keys\n",
        "        else:\n",
        "            return selected_examples\n",
        "\n",
        "\n",
        "class ExecutionBasedMultiSampleSelector(MultiSampleSelector):\n",
        "    def __init__(self, paths, split='dev', execution_type=None):\n",
        "        super().__init__(paths, split=split)\n",
        "        self.execution_type = execution_type\n",
        "        for i, path in enumerate(self.paths):\n",
        "            if execution_type == 'mbpp':\n",
        "                execute_mbpp_google_folder(path)\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    f'Execution type {execution_type} not supported.')\n",
        "            idx = 0\n",
        "            while os.path.exists(f'{path}/{split}-{idx}.exec.pkl'):\n",
        "                for j, execution_result in enumerate(pickle.load(open(f'{path}/{split}-{idx}.exec.pkl', 'rb'))):\n",
        "                    self.data[i, idx][j]['execution_result'] = execution_result\n",
        "                idx += 1\n",
        "            idx = 0\n",
        "            while os.path.exists(f'{path}/{split}-{idx}.execfull.pkl'):\n",
        "                for j, execution_result in enumerate(pickle.load(open(f'{path}/{split}-{idx}.execfull.pkl', 'rb'))):\n",
        "                    self.data[i, idx][j]['execution_result_full'] = execution_result\n",
        "                idx += 1\n",
        "            idx = 0\n",
        "            while os.path.exists(f'{path}/{split}-{idx}.execfullpass.pkl'):\n",
        "                for j, execution_result in enumerate(pickle.load(open(f'{path}/{split}-{idx}.execfullpass.pkl', 'rb'))):\n",
        "                    self.data[i, idx][j]['execution_result_full_pass'] = execution_result\n",
        "                idx += 1\n",
        "\n",
        "\n",
        "class IntraMultiSampleSelector(MultiSampleSelector):\n",
        "    def __init__(self, paths, split='dev', hyper_params=None):\n",
        "        super().__init__(paths, split=split, hyper_params=hyper_params)\n",
        "        # to normalize log_avg,  just take e^x, to normalize mbr, we just take mbr/argument\n",
        "        # already try to have it normalized\n",
        "    # current and max = np array of normalized bleu, mbr, log_avg()\n",
        "\n",
        "    def greater(self, current, max):  # potentially make contextual\n",
        "        #print(self.hyper_params)\n",
        "\n",
        "        # diff is array of differences ranging from [-1, 1]\n",
        "\n",
        "        # multiply by weights in linear interpolation then sum\n",
        "        mult = np.dot(current - max, self.hyper_params)\n",
        "        # print(current)\n",
        "        # print(max)\n",
        "        # print(mult)\n",
        "        if mult > 0:\n",
        "            return True\n",
        "        elif mult == 0:\n",
        "            return (current[2] > max[2])\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def select(\n",
        "        self,\n",
        "        ids=None,\n",
        "        key_extractor=None,\n",
        "        second_key_extractor=None,\n",
        "        return_keys=False, interpolation=False\n",
        "    ):\n",
        "        if ids is None:\n",
        "            ids = self.data.keys()\n",
        "        elif isinstance(ids, int):\n",
        "            ids = [(i, j) for i in set(x[0] for x in self.data.keys())\n",
        "                   for j in range(ids)]\n",
        "        ids = list(sorted(ids))\n",
        "        ids_length = len(ids)\n",
        "        id_set = set(ids)\n",
        "        sample_keys = collections.defaultdict(list)\n",
        "        print(f'Selecting Samples from IDs: {ids}')\n",
        "        n_examples = len(self.data[ids[0]])\n",
        "\n",
        "        selected_examples = list()\n",
        "        # 500 examples, where if we have 20 in the id set, we look at each id at self.data[id][length = 500]\n",
        "        for i in range(n_examples):\n",
        "            #print(i)\n",
        "            max_key = None\n",
        "            selected_item = None\n",
        "            for idx in id_set:\n",
        "                item = self.data[idx][i]\n",
        "\n",
        "                first_keys = list()\n",
        "                for grndtruth_idx in ids:\n",
        "                    # i can be up to 500 and grndtruth_idx is the hyperparameter we choose\n",
        "                    grndtruth_item = self.data[grndtruth_idx][i]\n",
        "\n",
        "                    key = key_extractor(item, grndtruth_item)\n",
        "                    first_keys.append(key)\n",
        "                # first key is the number that match with it in the sample (up to 20 etc. ) and second value is the log prob\n",
        "                first_key = sum(first_keys)\n",
        "                second_key = second_key_extractor(\n",
        "                    item) if second_key_extractor is not None else 0\n",
        "                current_key = (first_key, second_key)\n",
        "                item['mbr_key'] = current_key\n",
        "\n",
        "                # create interpolation value which are the values normalized from 0 to 1\n",
        "                # TODO: ADD MBR TOKEN BLUE\n",
        "                \n",
        "                if interpolation:\n",
        "                    token_bleus = list()\n",
        "                    char_bleus = list()\n",
        "                    token_meteor = list()\n",
        "                    for grndtruth_idx in ids:\n",
        "                        # i can be up to 500 and grndtruth_idx is the hyperparameter we choose\n",
        "                        grndtruth_item = self.data[grndtruth_idx][i]\n",
        "\n",
        "                        token_key = token_bleu_selection_function(item, grndtruth_item)\n",
        "                        token_bleus.append(token_key)\n",
        "                        \n",
        "                        char_key = bleu_selection_function(item, grndtruth_item)\n",
        "                        char_bleus.append(char_key)\n",
        "\n",
        "                        token_meteor_key = m(item, grndtruth_item)\n",
        "                        token_meteor.append(token_meteor_key)\n",
        "\n",
        "                    # first key is the number that match with it in the sample (up to 20 etc. ) and second value is the log prob\n",
        "                    token_bleus = sum(token_bleus)\n",
        "                    char_bleus = sum(char_bleus)\n",
        "                    token_meteor = sum(token_meteor)\n",
        "                    \n",
        "                    item['token_bleu'] = token_bleus\n",
        "                    item['char_bleu'] = char_bleus\n",
        "                    item['meteor'] = token_meteor\n",
        "                else:\n",
        "                    item['token_bleu'] = 0\n",
        "                    item['char_bleu'] = 0\n",
        "                    item['meteor'] = 0\n",
        "                    \n",
        "                item['interpolation'] = np.array(\n",
        "\n",
        "                    [item['token_bleu']/ids_length, item['char_bleu']/ids_length, item['meteor']/ids_length, first_key/ids_length, np.exp(item['avg_logprob'])])\n",
        "                if interpolation:\n",
        "\n",
        "                   \n",
        "                    \n",
        "                    current_key = (item['interpolation'],)\n",
        "                    # now need to write the case where interpolation = true\n",
        "                    sample_keys[idx].append(current_key)\n",
        "                    if max_key is None or self.greater(current_key[0], max_key[0]):\n",
        "                        max_key = current_key\n",
        "                        selected_item = item\n",
        "                else:\n",
        "                    sample_keys[idx].append(current_key)\n",
        "                    # should try ranking system but must ensure that rank 1 is actually given as n = size\n",
        "                    if max_key is None or current_key > max_key:\n",
        "                        max_key = current_key\n",
        "                        selected_item = item\n",
        "            assert selected_item is not None\n",
        "            selected_examples.append(selected_item)\n",
        "\n",
        "        if return_keys:\n",
        "            return selected_examples, sample_keys\n",
        "        else:\n",
        "            return selected_examples\n",
        "\n",
        "\n",
        "class ExecutionBasedIntraMultiSampleSelector(IntraMultiSampleSelector):\n",
        "    def __init__(self, paths, split='dev', execution_type=None, hyper_params=None):\n",
        "        super().__init__(paths, split=split, hyper_params=hyper_params)\n",
        "        self.execution_type = execution_type\n",
        "        for i, path in enumerate(self.paths):\n",
        "            if execution_type == 'mbpp':\n",
        "                execute_mbpp_google_folder(path)\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    f'Execution type {execution_type} not supported.')\n",
        "            idx = 0\n",
        "            while os.path.exists(f'{path}/{split}-{idx}.exec.pkl'):\n",
        "                for j, execution_result in enumerate(pickle.load(open(f'{path}/{split}-{idx}.exec.pkl', 'rb'))):\n",
        "                    self.data[i, idx][j]['execution_result'] = execution_result\n",
        "                idx += 1\n",
        "            idx = 0\n",
        "            while os.path.exists(f'{path}/{split}-{idx}.execfull.pkl'):\n",
        "                for j, execution_result in enumerate(pickle.load(open(f'{path}/{split}-{idx}.execfull.pkl', 'rb'))):\n",
        "                    self.data[i, idx][j]['execution_result_full'] = execution_result\n",
        "                idx += 1\n",
        "            idx = 0\n",
        "            while os.path.exists(f'{path}/{split}-{idx}.exec.codexcases.pkl'):\n",
        "                for j, execution_result in enumerate(pickle.load(open(f'{path}/{split}-{idx}.exec.codexcases.pkl', 'rb'))):\n",
        "                    self.data[i, idx][j]['execution_result_codexexec'] = execution_result\n",
        "                idx += 1\n",
        "            idx = 0\n",
        "            while os.path.exists(f'{path}/{split}-{idx}.execfullpass.pkl'):\n",
        "                for j, execution_result in enumerate(pickle.load(open(f'{path}/{split}-{idx}.execfullpass.pkl', 'rb'))):\n",
        "                    self.data[i, idx][j]['execution_result_full_pass'] = execution_result\n",
        "                idx += 1\n",
        "\n",
        "\n",
        "\"\"\"equivalence checking functions\"\"\"\n",
        "# base equavalence checking function\n",
        "\n",
        "\n",
        "def single_exec_result_matching(exec_x, exec_y, good_execution_result):\n",
        "    try:\n",
        "        if exec_x[0] == good_execution_result and exec_y[0] == good_execution_result and exec_x[1] == exec_y[1]:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "\n",
        "# first assertion call matching\n",
        "def execution_selection_function(x, y, good_execution_result=0):\n",
        "    exec_x, exec_y = x['execution_result'], y['execution_result']\n",
        "    return single_exec_result_matching(exec_x, exec_y, good_execution_result)\n",
        "\n",
        "\n",
        "# just executability checking\n",
        "def executability_selection_function(x, good_execution_result=0):\n",
        "    exec_res = x['execution_result']\n",
        "    return exec_res[0] == good_execution_result\n",
        "\n",
        "\n",
        "def bleu_selection_function(x, y):\n",
        "    return sentence_bleu([[ch for ch in x['trg_prediction']]], [ch for ch in y['trg_prediction']])\n",
        "\n",
        "\n",
        "def token_bleu_selection_function(x, y):\n",
        "    return sentence_bleu([x['trg_prediction'].split()], y['trg_prediction'].split())\n",
        "\n",
        "def meteor_selection_function(x,y): \n",
        "    return m([ch for ch in x['trg_prediction']], [ch for ch in y['trg_prediction']]) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    select and evaluate a group in batch\n",
        "    required keys:\n",
        "        data_split: 'train', 'dev' or 'test'\n",
        "        temperature: 0.1 .. 1.0\n",
        "        criterion: 'mbr_exec' ... see full options in the function\n",
        "        data_path: root data path for the task\n",
        "        n_samples: number of candidates\n",
        "        rand_seed: random seed for one experiment\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def select_mbpp(args, return_selected=False, return_selector=False, hyper_params=None):\n",
        "    data_split, temperature, criterion, data_path, n_samples, rand_seed = args\n",
        "    mbpp_good_execution_result = 0\n",
        "    data_path = f'{data_path}/seed-*/**/*-{temperature}/'\n",
        "    secondary_key_function = None\n",
        "    interpolation = False\n",
        "    if criterion == 'mbr_exec':\n",
        "        selector = ExecutionBasedIntraMultiSampleSelector(\n",
        "            data_path, data_split, 'mbpp')\n",
        "\n",
        "        def sample_selection_function(x, y): return execution_selection_function(\n",
        "            x, y, mbpp_good_execution_result)\n",
        "\n",
        "        def secondary_key_function(x): return x['sum_logprob']\n",
        "    elif criterion == 'interpolation_mbr_exec':\n",
        "        selector = ExecutionBasedIntraMultiSampleSelector(\n",
        "            data_path, data_split, 'mbpp', hyper_params=hyper_params)\n",
        "\n",
        "        def sample_selection_function(x, y): return execution_selection_function(\n",
        "            x, y, mbpp_good_execution_result)  # this can change because this is just mbr based on execution result -> add new ones that use different functions\n",
        "        interpolation = True\n",
        "\n",
        "    elif criterion == 'logprob':\n",
        "        selector = ExecutionBasedMultiSampleSelector(\n",
        "            data_path, data_split, 'mbpp')  # pre-execution for faster evaluation\n",
        "\n",
        "        def sample_selection_function(x): return x['sum_logprob']\n",
        "    elif criterion == 'avg_logprob':\n",
        "        selector = ExecutionBasedMultiSampleSelector(\n",
        "            data_path, data_split, 'mbpp')  # pre-execution for faster evaluation\n",
        "\n",
        "        def sample_selection_function(x): return x['avg_logprob']\n",
        "    elif criterion == 'mbr_bleu':\n",
        "        selector = ExecutionBasedIntraMultiSampleSelector(\n",
        "            data_path, data_split, 'mbpp')  # pre-execution for faster evaluation\n",
        "\n",
        "        def sample_selection_function(\n",
        "            x, y): return bleu_selection_function(x, y)\n",
        "    elif criterion == 'mbr_tokenbleu':\n",
        "        selector = ExecutionBasedIntraMultiSampleSelector(\n",
        "            data_path, data_split, 'mbpp')  # pre-execution for faster evaluation\n",
        "\n",
        "        def sample_selection_function(\n",
        "            x, y): return token_bleu_selection_function(x, y)\n",
        "    elif criterion == 'executability-logprob':\n",
        "        selector = ExecutionBasedMultiSampleSelector(\n",
        "            data_path, data_split, 'mbpp')\n",
        "\n",
        "        def sample_selection_function(x): return (executability_selection_function(\n",
        "            x, mbpp_good_execution_result), x['sum_logprob'])\n",
        "    elif criterion == 'executability-avglogprob':\n",
        "        selector = ExecutionBasedMultiSampleSelector(\n",
        "            data_path, data_split, 'mbpp')\n",
        "\n",
        "        def sample_selection_function(x): return (executability_selection_function(\n",
        "            x, mbpp_good_execution_result), x['avg_logprob'])\n",
        "    elif criterion == 'mbr_meteor': \n",
        "        selector = ExecutionBasedIntraMultiSampleSelector(\n",
        "            data_path, data_split, 'mbpp')  # pre-execution for faster evaluation\n",
        "\n",
        "        def sample_selection_function(\n",
        "            x, y): return meteor_selection_function(x, y)\n",
        "    elif criterion == 'executability-mbr_bleu':\n",
        "        selector = ExecutionBasedIntraMultiSampleSelector(\n",
        "            data_path, data_split, 'mbpp')  # pre-execution for faster evaluation\n",
        "\n",
        "        def sample_selection_function(x, y): return bleu_selection_function(x, y) * (1 - x['execution_result'][0]) * \\\n",
        "            (1 - y['execution_result'][0])\n",
        "    elif criterion == 'executability-mbr_tokenbleu':\n",
        "        selector = ExecutionBasedIntraMultiSampleSelector(\n",
        "            data_path, data_split, 'mbpp')  # pre-execution for faster evaluation\n",
        "        def sample_selection_function(x, y): return token_bleu_selection_function(x, y) * (1 - x['execution_result'][0]) * \\\n",
        "            (1 - y['execution_result'][0])\n",
        "    else:\n",
        "        raise ValueError(f'Unknown criterion: {criterion}')\n",
        "    id_keys = list(selector.data.keys())\n",
        "    random.seed(rand_seed)\n",
        "    ids = random.sample(id_keys, n_samples)\n",
        "\n",
        "    if secondary_key_function is not None:\n",
        "        selected = selector.select(\n",
        "            ids, sample_selection_function, secondary_key_function, interpolation=interpolation)\n",
        "    else:\n",
        "        selected = selector.select(\n",
        "            ids, sample_selection_function, interpolation=interpolation)\n",
        "    if return_selector:\n",
        "        return selector\n",
        "    elif return_selected:\n",
        "        return selected\n",
        "    else:\n",
        "        result = evaluate_google_mbpp(selected, '/content/drive/Shareddrives/COS484FinalProject/data/mbpp/mbpp.jsonl', 'test', gridSearch=True)\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run this to test interpolation\n",
        "\n",
        "hyper_params=np.array([-0.5, 0.5, -0.5, 100, 2])\n",
        "codegen_location = '/content/drive/Shareddrives/COS484FinalProject/data/mbr-2B/mbpp'\n",
        "codex_location = '/content/drive/Shareddrives/COS484FinalProject/codexData/mbpp/mbpp'\n",
        "select_mbpp(('test', 0.3, 'interpolation_mbr_exec',\n",
        "            codegen_location, 2, 0),hyper_params=hyper_params)"
      ],
      "metadata": {
        "id": "ieun1wwNMd1P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}